TrainerArgs.train_batch_size = 128
TrainerArgs.eval_batch_size = 128
TrainerArgs.log_interval = 50
TrainerArgs.eval_interval = 100
TrainerArgs.profile = False
TrainerArgs.profile_step_start = 50
TrainerArgs.profile_step_end = 80
TrainerArgs.max_train_iters = 20000
TrainerArgs.max_eval_iters = 700 #
TrainerArgs.pipeline_type = "none"
TrainerArgs.enable_balanced_shuffler = True  # Enable balanced batch shuffling for load balancing

TrainerArgs.top_k_for_generation = 20 # beam search width
TrainerArgs.eval_metrics = ('NDCG@10', 'Recall@10',  'HitRate@10') # evaluation metrics

DatasetArgs.dataset_name = 'amzn_beauty'
DatasetArgs.max_history_length = 30 # item-wise
DatasetArgs.max_candidate_length = 1 # loss on history if 0, else only on candidate.
DatasetArgs.dataset_type_str = "sid_sequence_dataset"
DatasetArgs.sequence_features_training_data_path = "./tmp_data/amzn/beauty/training/training_22363.parquet" #"./tmp_data/amzn/beauty/training/22363.parquet"
DatasetArgs.sequence_features_testing_data_path = "./tmp_data/amzn/beauty/evaluation/eval_22363.parquet" # "./tmp_data/amzn/beauty/train_test_split/augmented_all_101605.parquet" # augmented_test_16996
DatasetArgs.item_to_sid_mapping_path = "./tmp_data/amzn/beauty/item-sid-mapping.pt"
DatasetArgs.shuffle = False
DatasetArgs.num_hierarchies = 4
DatasetArgs.codebook_sizes = [256, 256, 256, 256] # embedding vocab size for each hierarchy, the last one is used for de-duplication

NetworkArgs.num_layers = 4
NetworkArgs.num_attention_heads = 6
NetworkArgs.hidden_size = 128 # embedding dim
# per head dim
NetworkArgs.kv_channels = 64
NetworkArgs.share_lm_head_across_hierarchies = True

OptimizerArgs.optimizer_str = 'adam'
OptimizerArgs.learning_rate = 1e-3

